##### Question

1. 如何理解语言的前后关系？



##### formula induction

$$
\begin{flalign*}
RNN:&h_t=f(h_{t-1},x_t)=f(z_t)=f(Uh_{t-1}+Wx_t+b)\\
&z_{t+1}=Uh_t+Wx_{t+1}+b&&\\
LSTM:&c_t=i_{t}\odot x_t+h_{t-1}\odot f_{t}
\end{flalign*}
$$

* BPTT

$$
\begin{flalign*}
\delta_{t,k}&=\frac {\partial L_t}{\partial z_k}=\frac{\partial L_t}{\partial z_{k+1}}\frac {z_{k+1}}{\partial h_{k}}\frac {h_{k}}{z_k}\\
&=\delta_{t,k+1}Uf'(z_{k})\\
\frac {\partial L_t}{\partial u_{i,j}}&=\sum_{k=1}^t\sum_{t}\frac{\partial z_{k,l}}{\partial}
\end{flalign*}
$$

* LSTM

$$
\begin{flalign*}
&c_t=f_t\odot c_{t-1}+i_t\odot \widetilde c_t\\
&h_t=o_t\odot tanh(c_t)\\
&where f_t,i_t,o_t are\ forget,input,output gate\\
&\widetilde c_t=tanh(W_cx_t+U_ch_{t-1}+b_c)\\
&where
\end{flalign*}
$$



##### nndl笔记

1. 具有记忆能力的3种网络的区别

   1. 延时神经网络=前向神经网络+前驱神经元历史信息
   2. 有外部输入的非线性自回归模型=外部历史输入+自身历史输出
   3. 循环神经网络=外部输入+前驱神经元历史(>>自身历史输出)

   * 均为处理序列式输入

2. 输出不仅与当前输入有关，还与之前的输入与输出有关

3. 通用近似定理拓展>>一个有足够数量的sigmoid神经元的循环神经网络可以模拟非线性动力系统

   $s_t=g(s_{t-1},x_t)\quad y_t=o(s_t)$，空间系统随时间的变换的函数表达

   上面这个神经网络是图灵-完备的，近似可以执行任何可计算问题

4. 应用场景、序列-标签、序列-序列(同步-只和当前输入、之前隐状态有关)、序列-序列(异步-编码器-译码器)

5. 参数优化、老实求导——**随时间反向传播算法* 

6. 自回归网络的问题-梯度消失、梯度爆炸

7. 长短期记忆网络和门控神经网络像OS中的软件定义门

   $z_t,(1-z_t)$处理历史与当前，$r_t\odot$选择历史

8. 如何增强循环神经网络的表达能力

   * 增加深度>>1. 简单堆砌循环网络，
     2. 为处理后续时刻的信息，采用双向循环网络
   * 变换结构——1. 采用递归神经网络，特例=处理序列的循环神经网络
     2. 采用图结构——所有结点同步更新，$o_t=g(\{h_T^{(v)}|v\in V\})$,效率不如异步的循环/递归网络

![image-20230323073447338](https://raw.githubusercontent.com/cl626/Image/master/Picgo/image-20230323073447338.png)

##### code理解

* ~~如何理解语言的前后关系？~~如何从当前单词预测下一单词，

1. 每一个单词，不仅与前一个单词有关，可能还和前若干个单词relate；

* RNN，每一个输出不仅与当前输入有关，还与前驱输入有关

2. when预测词元，上下文关系这么复杂，随机确定相关前驱吗？

##### RNN建立

2. 文本处理——词元→索引，==优化==→高频词索引前？

3. 语言模型

* 语音识别中歧义可通过语言识别解决

* 考虑上下文关系，粗暴法=bayes定理，$P(learning|deep)=\frac {n(deep,learning)}{n(deep)}$问题如下

> 1. 存储有笛卡尔积组合并统计，👉组合爆炸
> 2. 完全忽略单词意思？——单词意思的作用？——存在某种可能性更大的(同距离或不同距离更近的)估计，但不符合上下文意思
> 3. 估计了大部分未出现的longwords 

* Natural language statics

| 词元数 |                                  illustration                                  |
| --------- | -------------------------------------------------------------------- |
| 1          | $n_i 正比于\frac 1 {i^\alpha},logn_i=-\alpha logi +c$ |
| 2          | the same ,$\alpha$ smaller                                        |
| 3          | the same ,$\alpha$ smaller                                        |

> n元组数量<10，表明小n可以近似覆盖所有出现2次以上的词对

* 对长序列数据，可以自定义偏移，小批量随机采样

4. 循环神经网络

* $H_{t-1}$ captures history til current time step，==啥也不知道就用H~t-1~瞎拟合，√==

![image-20230326111536956](https://raw.githubusercontent.com/cl626/Image/master/Picgo/image-20230326111536956.png)

* Perplexity

![image-20230326110932778](https://raw.githubusercontent.com/cl626/Image/master/Picgo/image-20230326110932778.png)

* $\frac 1 n\sum_{t=1}^n logP(x_t|x_{t-1},...,x_1)$为Cross_Entropy function，指数为频度的估计出现概率对数之和

5. RNN from zero

* 独热编码

> 一种特殊的比特组或向量，该字节或向量里仅容许其中一位为1，其他位都必须为0，独冷

* 为什么要用独热码?

> 总之，独热码是一种方便表示离散变量的方法，在 RNN 中用于将离散变量表示成固定长度的向量，方便输入和输出。
>
> 把输入转为独热吗的形式，==加速计算==

* model

![image-20230326122040633](../../Users/c1826/AppData/Roaming/Typora/typora-user-images/image-20230326122040633.png)

> 中间层Ht的维数自定

* 网络和前向定义，参数&状态初始化，
* 训练，优化
* 梯度裁剪，$g_{\theta}=min(\frac{\theta}{||g_{\theta}||},\frac{g_{\theta}}{||g_{\theta}||})||g_{\theta}||$


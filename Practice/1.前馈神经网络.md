##### Question

1. 进行反向传播时，各层之间有更新吗？



1. 函数逼近定理确保精确参数存在，但可能过拟合
2. 激活函数---神经元---前向传播
3. 误差函数---反向传播---调整激活函数参数
4. 求导&梯度计算、非凸优化和梯度消失
5. 机器学习与深度学习的区别

![image-20221229180614810](C:/Users/c1826/AppData/Roaming/Typora/typora-user-images/image-20221229180614810.png)

* 常用激活函数

1. Gauss Error Linear U激活函数，二分类

2. MaxOut $g(z)_i=\underset {j\in G^{(i)}}{maxz_j},z_j=w_jx+b$

   足够大的k,maxout可以以任意的精确度来approximate Convex functions

3. Logistic=$\frac 1{1+exp(-x)}$，Softmax=$\frac{e^{w_ix_i+b_i}}{\sum_{j=1}^n e^{w_jx_j+b_j}}$（自带线性变换)
4. Tanh=$\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}$
5. ReLU=max(0,x) ,最常用
6. ELU=max(0,x)+min(0,y(exp(x)-1))
7. SoftPlus=log(1+exp(x))   ,f'(x)=Logistic

* 
  $W^l是l-1层到l层的权重矩阵$

* 没有激活函数的forward neural network is  linear classifier

* ==万能近似定理==：具有线性输出层和至少一层具有挤压性质激活函数的隐藏层，只要基于足够多的隐藏单元，可以近似Borel 可测函数，

  <font color=green>在R^n^的有界闭集上的任意连续函数 is Borel fathomable</font>

* Deeper， less unit ，less generalizaiton error
* 存在函数族能够在网络的深度>d时被高效近似，否则需要原大于之前的模型

##### Defect

1. 		2. 过拟合

##### 多分类

1. $L(y,\overline y)=-y^Tlog\overline y=-z_c+\underset{i\leq k} max(z_i)$

2. 经验风险对应极大似然估计，结构风险对应最大后验估计
3. 随机梯度下降在pytorch中的实现？——Designed at definition ,SGD(stochastic gradient decline)

4. complex composed function ? 

5. 标值-向量-张量

6. $\frac {\partial a(\vec z^{(l)})}{\partial \vec z}=diag(f'(z))$




调用net，定义函数，不应该是问题

Am I reciting ? 

##### Question

1. Convolutionary function's 输出，也是个向量

> 卷积网络在图像和语音识别，比其他神经网络效果更好，更少的参数，更高的性能
>
> 这仅仅是因为对应数据集爆炸，卷积网络**能在短时间内提取图像特征**

1. 卷积是倒序的Hadamard积

2. > 全连接层的每一个神经元必须对应所有输入，
   >
   > 为了减少参数，让属于神经元的视野作为卷积层神经元的输入，
   >
   > 经过卷积和激活函数运算，作为卷积层的输出特

3. 和前馈neural network一样，卷积neural network一样有反向传播算法

   $z^{l}=w^{(l)}\otimes a^{(l-1)}+b^{(l)}，y_{ij}=\sum_{u=1}^{U}\sum_{v=1}^{V}w_{uv}x_{i+u-1,j+v-1}=(W\otimes X)_{ij}=rot180(W)*X$

   所以实际上是互相关层而非卷积层

4. 卷积神经网络的输出特征可以不由所有输入特征决定，而采用连接表形式

   $Y=f(\underset{d,T_{p,d}=1}{\sum} W^{p,d}\otimes X^d+b^p)$

5. 可以用神经网络构造$f(x,\theta)$逼近h(x)-x，再用$f(x,\theta)+x$逼近h(x)>>残差网络

6. 其他卷积，转置卷积>>用来把低维特征映射为高维特征，可通过两端补k-1个0再做卷积实现，

   和卷积本质上都可看做仿射变换，互为形式上的转置，

   微步卷积：步长S<1,得到(D+1)*(M-1)+K维的向量，D每两个元素之间插入0个数，K两端补0个数

   空洞卷积：在输入特征每两个元素之间添D-1个空洞

7. CNN与MLP的区别？

* 应该是卷积和池化，来降低数据数量，再用MLP

##### talk session

1. 4维(for picture)：batch ,  RGB(dim=3,[[0-255]]{3})，hight , width 

2. boring output size；(Y-F)/S+1，

   卷积层：nn.Conv2d(in_channel，out_channel，padding，dilation(<font color=orange>input interval</font>)，stride，kernel_size，group(<font color=green>group|in_channels</font>),bias)

   池化层：nn.MaxPool2d()，nn.AvgPool2d()——

   nn.AdaptiveMaxPool2d()，remains channel 

3. 卷积的动机，

   1. 稀疏连接

   2. 平移不变性

4. 卷积和全局池化的过程

* 卷积：every patch：input(ci,ih,iw)，kernel_size(co,ci,kh,kw)，output(co,oh,ow)
* 池化：局部池化，kernel后→kernel后：(co,oh,ow)→(co,oh’,ow‘)，wish 特征图更小
* 全局池化：(co,oh,ow)—nn.AdaptiveMaxPool2d((1))→(co,1)
* (oh,ow)→(1) equals full-connected layer

5. 1×1kernel相当于对通道元素的全连接层，全局池化=把一个二维张量视为(转化为)一个变量的全连接层(c~i~-连接层权重)，(iw,1)=对行整体的全连接层，(ih,1)=对列整体的全连接层
6. 需要对卷积参数进行梯度下降吗？
7. 转置卷积/反卷积？
8. what's the meaning of Batch?  

###### advanced CNN

1. VGG，（Conv+ReLU）*k+MaxPool，automation（更深的网络，smaller convolutional kernel)

2. NiN，NiN block 输出通道数=标签类别的数量，全局平均 replace 全局最大，综合特征

3. GoogleNet，Inception block——先降低维数，用最简单的1*1提高这个过程的速度，再做卷积，全局平均

4. ResNet：拟合的是残差f(x)=h(x)-x，可用1*1降通道数

   ==Background==：深层网络训练集的错误率会上升，

   为什么会起作用呢——(((conv(2\*3)+relu)\*2+x)+relu)*152

5. DenseNet一点可解释性：![image-20230413172109745](../../Users/c1826/AppData/Roaming/Typora/typora-user-images/image-20230413172109745.png)

* end in 全连接层+softmax(多分类)
* ==玄学，万物皆数==

* NN science basis

##### param optimization

* 集成学习
* instance normalization：
* IN：归一化每个批每个通道
* BN：add 可学习参数$\gamma$和$\beta$，保持表达能力，归一化不同批的同一通道的层
* LN，用于批中维数不一致的序列式数据，归一化一个批全部通道

* group normalization: 将channels 化为多个组，单组一个channel为BN

![img](https://raw.githubusercontent.com/cl626/Image/master/Picgo/zwf6meyt99.png)

> 上图中有四种Normalization的方法。就先从最简单的Instance Normalization开始分析：
>
> - IN：仅仅对每一个图片的每一个通道最归一化。也就是说，对【H，W】维度做归一化。假设一个特征图有10个通道，那么就会得到10个均值和10个方差；要是一个batch有5个样本，每个样本有10个通道，那么IN总共会计算出50个均值方差；
> - LN：对一个特征图的所有通道做归一化。5个10通道的特征图，LN会给出5个均值方差；
> - GN：这个是介于LN和IN之间的一种方法。假设Group分成2个，那么10个通道就会被分成5和5两组。然后5个10通道特征图会计算出10个均值方差。
> - BN:这个就是对Batch维度进行计算。所以假设5个100通道的特征图的话，就会计算出100个均值方差。5个batch中每一个通道就会计算出来一个均值方差。

##### 训练目标和方式

###### supervision pre-training

* FitNets

1. 教师和学生网络:

* 教师网络宽而浅，学生网络窄而深

###### 课程学习

1. 先学习简单的

* 相当于对简单的组合参数的拟合

###### 对抗学习

1. 在对抗扰动的训练集上训练网格，减少独立同分布的测试机的错误率

2. 真品如何变为假品？ ——GRU循环神经网络

###### 迁移学习

##### 数据集增强

* 翻转，
* 变换+采样

1. 缩放到5个尺度，
2. 每个尺度2*(4+1)

* 变色
* append noise
* 标签平滑

##### 如何理解学习率和梯度迭代方法的改进

1. 

##### 变分自编码器

